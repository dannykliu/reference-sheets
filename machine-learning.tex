\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amssymb, amsmath, geometry, nopageno, xfrac, nicefrac, mathtools}
\usepackage{graphicx, multicol}
\graphicspath{ {./images/} }

\setlength{\textheight}{600pt}

\DeclareMathOperator{\im}{Im}
\DeclareMathOperator{\PProjection}{proj}


\newcommand{\Z}{\mathbb{Z}}
\renewcommand{\P}{p}
\newcommand{\E}{\mathbb{E}}
\newcommand{\V}{\mbox{Var}}
\newcommand{\C}{\mbox{Cov}}
\renewcommand{\H}{\mathcal{H}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\R}{\mathbb{R}}
\newcommand\inner[2]{\langle #1, #2 \rangle}
\newcommand{\vct}{\mathbf}
\newcommand{\PProj}[2][]{\PProjection_{\vct{#1}}\vct{#2}}

\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin} 
\DeclareMathOperator{\sgn}{sgn} 
\DeclareMathOperator{\rank}{rank} 
\DeclareMathOperator{\diag}{diag} 
\DeclarePairedDelimiter\abs{\lvert}{\rvert}
\DeclarePairedDelimiter\norm{\lVert}{\rVert}


\title{Machine Learning Reference Sheet}
\author{By Danny Liu}
\date{}

\begin{document}
\maketitle

\section{Basic Knowledge}
Assume $(x^{(i)}, y^{(i)}) \sim p(x,y)$ is generated iid such that $f(x^{(i)}) = y^{(i)}$. ML tries to approximate $f$ \\
Parametric models assume $D \sim p(D; \theta)$ and so $D$ is no longer needed after $\theta$ has been estimated \\
Non-parametric (memory-based) models store $D$ and interpolate them later for prediction \\
Regression, linear svm, generative models (parametric) and d-trees, kNN, rbf svm (nonparametric) \\
Feature importances, rf, regression, svm, \\
The $\ell_p$ norm $\norm{x}_p = \big(\sum\limits_{i=1}^n \abs{x_i}^p \big)^{\sfrac{1}{p}}$ implicitly defines an inner product, where $\norm{x}_2 =  \sqrt{x^Tx}$ 

\section{Evaluating Performance}
blah

\section{Multiclass Classification}
blah

\section{Principal Component Analysis}
Assume data approximately lies on lower dimensional subspace

\section{Ensemble Methods}
blah

\section{Decision Trees}
Leaf nodes predict $Y$ or $p(Y|X \in \mbox{leaf})$. Training time $O(dn\log n)$ \\ 
Internal nodes test one feature $X_i$ against threshold and each branch selects value for $X_i$ \\
Decision trees are rule based classifiers that divide feature space into axis-parallel (hyper-)rectangles \\
Reduce overfitting with max\_depth, max\_features, min\_samples\_split, min\_samples\_leaf, etc...

\subsection{Entropy}
$H(X) = -\sum \limits_{i=1}^n p(X = i)\log_2 p(X=i)$ measures impurity/uncertainty of random variable \\
$H(Y| X) = \sum \limits_{v \in X}p(X = v)H(Y | X = v)$ gives conditional entropy of $Y$ given $X$ \\
$I(Y, X) = H(Y) - H(Y|X) = I(X, Y)$ gives info-gain/mutual information of $Y$ and $X$ \\
Assuming sender and receiver know distribution, how many bits on average to transmit one symbol? \\
${p(X = 1) = 1 \implies H(X) = 0\mbox{, } p(X = 1) = 0.5 \implies H(X) = 1\mbox{, } p(X = 1) = 0.9 \implies H(X) = 0.136}$ 
\subsection{ID3 Algorithm}
Starting at root, if entropy is 0 or all feature values are equal, create leaf nodes \\
Else, choose feature $X_i$ and threshold $c$ based on largest info-gain / minimum conditional entropy \\
For each value of $X_i$, create new descendent, sort training examples to descendent nodes and recurse

\section{k-Nearest Neighbors}
For kNN, find $k$ nearest neighbors of $x$ and choose majority label as predicted class \\
$k \propto \lambda$, as if $k$ is too small then we are overfitting to the noise. $k=1 \implies$ 100\% training accuracy \\
Curse of Dimensionality: Volume increases exponentially with dimensions so data becomes sparse

\newpage
\section{Linear Regression}
Assume $\epsilon^{(i)} \sim \N(0, \sigma^2)$, $y^{(i)} = \theta^Tx^{(i)} + \epsilon^{(i)}$, then $p(\epsilon^{(i)}) = \frac{1}{\sqrt{2 \pi} \sigma} \exp \big ({\frac{-(y^{(i)} - \theta^T x^{(i)})^2}{2\sigma^2}} \big)$ gives us \\
$\ell(\theta) = \log \prod\limits_{i=1}^n p(y^{(i)} | x^{(i)} ; \theta) = \log \prod\limits_{i=1}^n \frac{1}{\sqrt{2 \pi} \sigma} \exp \big ({\frac{-(y^{(i)} - \theta^T x^{(i)})^2}{2\sigma^2}} \big) $ \\
\hspace*{0.577cm} $ = \sum\limits_{i=1}^n [-\log(\sqrt{2\pi}\sigma) - \frac{1}{2\sigma^2} (y^{(i)} - \theta^T x^{(i)})^2] = -n\log(\sqrt{2\pi}\sigma) - \frac{1}{\sigma^2} J(\theta) $ \\
$J(\theta) = \frac{1}{2} \sum\limits_{i=1}^n (h_\theta(x^{(i)}) - y^{(i)})^2 = \frac{1}{2} (X\theta - y)^T(X\theta - y)$ \\
$\nabla_\theta J(\theta) = X^TX\theta - X^Ty$, $\nabla_\theta^2 J(\theta)^T = X^TX \succeq 0$, $\frac{\partial J}{\partial \theta_j} = \sum\limits_{i=1}^n (h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)}$, $\frac{\partial J}{\partial \theta_j \partial\theta_k} = \sum\limits_{i=1}^n x_j^{(i)}x_k^{(i)}$ \\
Note that $\rank X = \rank X^T = \rank XX^T = \rank X^TX $, so $\theta$ always has a solution \\
$X^TX$ is singular when $X$ has dependent columns, but inverting is very costly $O(d^3)$ anyways

\section{Logistic Regression}
Assume $y|x ; \theta \sim \mbox{Bernoulli}(\phi)$, $\log \frac{\phi}{1-\phi} = \theta^T x$, then $\phi = \frac{1}{1+e^{-\theta^T x}} = h_\theta(x) = \sigma(\theta^Tx) $ gives us \\
$\ell(\theta) =\log \prod\limits_{i=1}^n p(y^{(i)} | x^{(i)} ; \theta) = \sum\limits_{i=1}^n y^{(i)}\log h_\theta(x^{(i)}) + (1-y^{(i)})\log (1-h_\theta(x^{(i)})) = - J(\theta)$ \\
$\nabla_\theta J(\theta) = X^T (\sigma(X\theta) - y)$, $\nabla_\theta^2 J(\theta)^T =  X^T \diag \sigma^\prime (X\theta)X \succeq 0$, $\frac{\partial J}{\partial \theta_j} = \sum\limits_{i=1}^n (h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)}$ \\
Note $\sigma(z) = \frac{1}{1+e^{-z}}$, $\sigma^\prime(z) = \sigma(z)(1-\sigma(z))$ and if $y \in \{-1, +1\}$, then $L(y, \hat y) =\log \sigma(y\hat y)$

\section{Softmax Regression}
Assume $y|x ; \boldsymbol \theta \sim \mbox{Cat}(\boldsymbol \phi)$, $\log\frac{\phi_j}{\phi_k} = \theta_j^Tx$, then $\sum\limits_{j=1}^k \phi_j = \sum\limits_{j=1}^k e^{\theta_j^Tx}\phi_k = 1 \implies \phi_j = \frac{e^{\theta_j^Tx}}{\sum_{l=1}^k e^{\theta_l^Tx}}$ gives us \\
$\ell(\boldsymbol \theta) = \log \prod\limits_{i=1}^n p(y^{(i)} | x^{(i)} ; \boldsymbol \theta) = \log \prod\limits_{i=1}^n \prod\limits_{j=1}^k (h_{\theta_j}(x^{(i)}) )^{1\{y^{(i)}_j = 1\}} = \sum\limits_{i=1}^n \sum\limits_{j=1}^k y_j^{(i)}\log h_{\theta_j}(x^{(i)}) = - J(\boldsymbol \theta)$ \\
$\nabla_{\theta_j} J(\boldsymbol \theta) = -\sum\limits_{i=1}^n \sum\limits_{j=1}^k y_j^{(i)}(1-h_{\theta_j}(x^{(i)}))x^{(i)}$, $\nabla_{\boldsymbol \theta}J(\boldsymbol \theta) = X^T (h_{\boldsymbol \theta}(X)-1)\odot y$ has dimension $d \times k$ \\
Note $\sigma(\boldsymbol z)_j = \frac{\exp(z_j)}{\sum_k \exp(z_k)}$, $\frac{\partial \sigma (\boldsymbol z)_j}{\partial z_i} = \sigma (\boldsymbol z)_i ( 1- \sigma (\boldsymbol z)_i)$ if $i = j$ and $\frac{\partial \sigma (\boldsymbol z)_j}{\partial z_i} = -\sigma (\boldsymbol z)_i\sigma (\boldsymbol z)_j$ if $i \neq j$

\section{Regularization}
Assuming the prior $\theta \sim \N(0, \lambda^{-1}I)$ is equivalent $\ell_2$ regularization in the MAP estimate \\
$\argmax\limits_\theta \log p(\theta | D) = \argmax\limits_\theta \log p(D | \theta) p(\theta) = \argmax\limits_\theta \log p(D| \theta) + \log p(\theta)$ \\
\hspace*{2.717cm} $ = \argmax\limits_\theta \log p(D| \theta) + \log \Big(\frac{1}{(2\pi)^{\sfrac{d}{2}} \abs{\lambda^{-1}I}^{\sfrac{1}{2}}}\exp{(-\frac{1}{2} \theta^T (\lambda^{-1}I)^{-1} \theta)}\Big) $ \\
\hspace*{2.717cm} $ =\argmax\limits_\theta  \log p(D| \theta) - \frac{\lambda}{2}\norm {\theta_{1:d}}_2^2$ \\
Assuming the prior $\theta_i \sim \mbox{Lap}(0, \frac{1}{\lambda})$ is equivalent to $\ell_1$ regularization in the MAP estimate \\
$\argmax\limits_\theta \log p(\theta | D) = \argmax\limits_\theta \log p(D| \theta) + \log \prod\limits_{i=1}^d \frac{1}{2 (\sfrac{1}{\lambda})}\exp (-\frac{\abs{ \theta_i}}{\sfrac{1}{\lambda}}) =  \argmax\limits_\theta \log p(D| \theta) -\lambda \norm{\theta_{1:d}}_1$ \\
$\ell_1$ regularization results in sparser solutions as it's more likely for the loss function to touch the axis aligned spikes first, but is not differentiable. Laplace prior also places more mass near $\theta_i = 0$ 

\newpage
\section{Perceptron}
Assume data is linearly separable by a hyperplane with normal vector $\theta$, then $h_\theta(x) = \sgn(\theta^Tx)$ \\
Recall distance from $(x_0, y_0, z_0)$ to plane is $D = \frac{ax_0 + by_0 + cz_0 + d}{\sqrt{a^2 + b^2 + c^2}}$ where $\vec n = (a, b, c)$ \\
Repeat until convergence for $i=1$ to $n$, if $y^{(i)}\theta^T x^{(i)} \leq 0$, then $\theta \leftarrow \theta + y^{(i)}x^{(i)}$ \\
Intuition is that $\theta$ tilts more towards $x^{(i)}$ if it was positive and more away if negative \\
$J(\theta) = \sum\limits_{i=1}^n \max(0, -y^{(i)} \theta^T x^{(i)})$, then for single example $-\nabla_\theta J_i(\theta) = y^{(i)}x^{(i)}$ \\
Geometric margin of $i$th example $\gamma^{(i)} = \frac{y^{(i)}(\theta^Tx + b)}{\norm \theta}$ is signed distance from $x^{(i)}$ to hyperplane
\subsection{Perceptron Convergence Theorem}
If data is linearly separable with margin $\gamma = \min\limits_i \gamma^{(i)}$ by unit-norm $\theta^*$ and $\norm {x^{(i)}} \leq R$ for all $i$, \\ then the perceptron converges after at most $\frac{R^2}{\gamma^2}$ updates.

\section{Support Vector Machines}
$\min\limits_{\theta, b, \xi} \frac{1}{2} \norm{\theta} ^2 + C\sum\limits_{i=1}^n\xi_i$ such that $y^{(i)}(\theta^Tx^{(i)} + b) \geq 1 - \xi_i$, where $\xi_i = \max(0, 1-y^{(i)}(\theta^Tx^{(i)} + b))$ \\
$\max\limits_\alpha \sum\limits_{i=1}^n \alpha_i - \frac{1}{2}\sum\limits_{i=1}^n\sum\limits_{j=1}^n \alpha_i \alpha_j y^{(i)}y^{(j)}\inner{x^{(i)}}{x^{(j)}}$ such that $0 \leq \alpha_i \leq C$ and $\sum\limits_{i=1}^n \alpha_i y^{(i)} = 0$ \\
From KKT conditions $\alpha_i = 0 \implies y^{(i)}(\theta^Tx^{(i)} + b) > 1$, $\alpha_i = C \implies y^{(i)}(\theta^Tx^{(i)} + b) < 1$ and $0 < \alpha_i < C \implies y^{(i)}(\theta^Tx^{(i)} + b) = 1$, where an upper bound of $C$ limits $\alpha_i$ to allow misclassifications \\
$J(\theta) = \sum\limits_{i=1}^n \xi_i = \sum\limits_{i=1}^n \max(0, 1-y^{(i)}(\theta^Tx^{(i)} + b))$ hinge loss is encoded in slack variables \\
$\theta = \sum\limits_{i=1}^n \alpha_i y^{(i)}x^{(i)}$, $\hat y = \sum\limits_{i=1}^n \alpha_i y^{(i)} \inner{x^{(i)}}{x} + b$, where $b = y^{(i)}(1 - \xi_i) - \theta^T x^{(i)}$ for $0 < \alpha_i < C$ \\
Since $\gamma \propto \frac{1}{\norm \theta}$, maximizing margin is equivalent to minimizing $\norm{\theta}^2$. Bias shifts $-\frac{b}{\theta_i}$ in $\hat{x_i}$ \\
Dual learns weights to s.vs and predicts test example by a weighted similarity to s.vs \\
$C \propto \frac{1}{\lambda}$, as (low $C$) maximize margin, more misclassified examples, less s.vs, lower model complexity \\
In practice, prefer low $C$ so training is faster. Train time $O(n^2d)$ rbf and $O(nd)$ linear

\subsection{Kernels}
Since prediction and optimization only depend on inner products, lets map $x^{(i)} \to \phi(x^{(i)})$ \\
Computing $\phi(x^{(i)})$ is inefficient, thus find implicit mapping (kernel) where $k(x^{(i)}, x^{(j)}) = \inner{\phi(x^{(i)})}{\phi(x^{(j)})}$ \\
Kernels redefine inner product as if we had found the corresponding mapping to a higher dimension \\
$k(x,z) = (1 + \inner{x}{z})^2 \implies \phi(x) = (1, x_1^2, x_2^2, \sqrt{2}x_1x_2, \sqrt{2}x_1, \sqrt{2}x_2)^T$ "maps" $x$ to quadratic space \\
If $k_1(x, z)$, $k_2(x, z)$ are valid kernels, then $k(x, z) = f(x)k_1(x, z)k_2(x, z) + k_2(x, z)$ is a valid kernel \\
$k(x, z) = \exp(-\gamma \norm{x-z}^2)$, $\gamma > 0$ places a Gaussian around every support vector, hence $\gamma \propto \frac{1}{\lambda}$ \\
$\exp(-\frac{1}{2}\norm{x-z}^2) = \exp(-\frac{1}{2}x^Tx)\exp(x^Tz)\exp(-\frac{1}{2}z^Tz)$, $\exp(x^Tz) = 1 + \frac{x^Tz}{1!} + \frac{(x^Tz)^2}{2!} + \ldots$ \\
$k(x,z) = \tanh(\gamma\inner{x}{z} + r)$ is sigmoid kernel and $k(x, z) = (\gamma\inner{x}{z} + r)^d$ is polynomial kernel

\subsection{Mercer's Theorem}
$k$ is a valid kernel $\iff$ $K \succeq 0$ for any $\{x^{(i)}\}_{i=1}^n$, where $K_{ij} = k(x^{(i)}, x^{(j)})  = \inner{\phi(x^{(i)})}{\phi(x^{(j)})}$ \\
$K \succeq 0 \implies K_{ij} = e_i^TPDP^Te_j = (e_i^TPD^{\frac{1}{2}})(D^{\frac{1}{2}}P^Te_j) = (D^\frac{1}{2}P^Te_i)^T(D^\frac{1}{2}P^Te_j) = \inner{\phi(x^{(i)}}{\phi(x^{(j)})}$ \\
$K_{ij} = \inner{\phi(x^{(i)}}{\phi(x^{(j)})} \implies x^TKx = \sum_{ij} x_ix_j K_{ij} = \inner{\sum_i x_i \phi(x^{(i)})}{\sum_j x_j \phi(x^{(j)})} \geq 0 $

\section{Generative Learning Algorithms}
Discriminative algorithms (d-trees, softmax, svms, kNN, k-means, etc..) try to learn $p(y|x)$ \\
Generative algorithms model $p(y)$ and $p(x|y)$ and then make predictions by maximizing the posterior probability $\argmax\limits_y p(y|x) = \argmax\limits_y \frac{p(x|y)p(y)}{p(x)} = \argmax\limits_y p(x|y)p(y)$ 

\subsection{Gaussian Discriminant Analysis}
Assume $y \sim \mbox{Bernoulli}(\phi)$, $x|y=0 \sim \mathcal{N}(\mu_0, \Sigma)$, $x|y=1 \sim \mathcal{N}(\mu_1, \Sigma)$, which gives us \\
$\ell(\phi, \mu_0, \mu_1, \Sigma) = \log \prod\limits_{i=1}^np(x^{(i)}, y^{(i)}; \phi, \mu_0, \mu_1, \Sigma) = \log \prod\limits_{i=1}^np(x^{(i)} | y^{(i)}; \mu_0, \mu_1, \Sigma)p(y^{(i)}; \phi)$, solving \\
$\phi = \frac{1}{n}\sum\limits_{i=1}^n 1\{y^{(i)} = 1\}$, $\mu_j = \frac{\sum_{i=1}^n 1\{y^{(i)} = j\} x^{(i)}} {\sum_{i=1}^n 1\{y^{(i)} = j\}}$, $\Sigma = \frac{1}{n}\sum\limits_{i=1}^n (x^{(i)} - \mu_{y^{(i)}})(x^{(i)} - \mu_{y^{(i)}})^T$ \\
By viewing $p(y=1|x) = f_1(x)$, we can write $p(y=1|x) = \frac{1}{1+\exp(-\theta^Tx)}$ where $\theta = f_2(\phi, \Sigma, \mu_0, \mu_1)$ \\
If $p(x|y)$ is Gaussian, then $p(y|x)$ is logistic. However, the assumption  $x|y = j \sim \mbox{Poisson}(\lambda_j)$ still leads to a logistic posterior. Hence GDA makes a stronger assumption that data is Gaussian \\
Note that while there are two mean vectors $(\mu_0, \mu_1)$, GDA typically shares the covariance matrix $\Sigma$

\subsection{Naive Bayes}
Assume $y \sim \mbox{Multinoulli}(\boldsymbol \phi)$ and $p(x|y) = \prod\limits_{i=1}^d p(x_i | y)$, then our prediction rule is simply \\
$\argmax\limits_y p(y | x) = \argmax\limits_y p(x|y)p(y) = \argmax\limits_y \prod\limits_{i=1}^d p(x_i | y) p(y)$ \\
If $x_i$ is categorial or continuous, then we can split it into $k_i$ Bernoullis by discretizing

\section{Optimization}
Batch gradient descent does one step using $n$ examples: $ \theta \leftarrow \theta - \alpha \nabla_\theta J(\theta)$ \\
Stochastic gradient descent does one step \textit{per} example: $\theta \leftarrow \theta - \alpha \nabla_\theta J_i(\theta)$ \\
Learning rate $\alpha_k = \frac{1}{1+k}$ where $k = \mbox{outer loop iteration}$ is typically a good choice

\subsection{Newton's Method}
Iteratively find better approximations for roots of differentiable function by $x_{n+1} = x_n - \frac{f(x_n)}{f^\prime(x_n)}$ \\
In optimization, find critical points of twice differentiable function by $\boldsymbol{x}_{n+1} = \boldsymbol{x}_n - \gamma \boldsymbol{H}f(\boldsymbol{x}_n)^{-1} \nabla f(\boldsymbol{x}_n) $ \\
Often converges in fewer steps than gradient descent, but computing $\boldsymbol{H}_f^{-1}$ is costly 

\subsection{Expectation Maximization}

Given a statistical model that generates observed $\{x^{(i)}\}_{i=1}^n$ and latent $\{z^{(i)}\}_{i=1}^n$ with parameters $\theta$, the marginal likelihood $p(X | \theta) = \sum\limits_{i=1}^n\log\sum\limits_{\mbox{ }z^{(i)}} p(x^{(i)}| z^{(i)})p(z^{(i)})$ is often intractable to solve directly \\
Begin with initial guess for model parameters. Then repeating until convergence \\
Set $r_{ij} = p(z^{(i)} = j | x^{(i)}; \theta)$ [E-step] (given $\theta$ guess distribution of $z^{(i)}$) \\
Solve $\theta^* = \argmax\limits_\theta \log p(X|\theta) =  \sum_{i}\sum_{j} \log p(x^{(i)} | z^{(i)} = j; \theta)r_{ij} $ [M-step] (MLE for $\theta$ given $z^{(i)}$) \\
EM repeatedly sets $\theta^* = \argmax\limits_\theta E[\ell(\theta) | Y]$ where $Y = Z|X; \theta$, guaranteed to find local optima \\
The [E-step] constructs $\ell(\theta_{t+1})$ that lower bounds $\ell(\theta)$ and the [M-step] maximizes $\ell(\theta_{t+1})$

\newpage
\section{Clustering}
Organize data into clusters such that there is high intra-cluster and low inter-cluster similarity 

\subsection{k-Means}
Initialize $k$ cluster centroids randomly ${\mu_1}, \ldots, {\mu_k} \in \R^d$, then repeating until convergence \\
for $i = 1, \ldots, n$, set $c^{(i)} = \argmin\limits_j \norm{{x^{(i)}}- {\mu_j}}^2$ minimizes $J$ w.r.t $ c$ \\
for $j = 1, \ldots, k$, set ${\mu_j} = \frac{\sum_{i=1}^n 1\{c^{(i)}=j \}{x^{(i)}}}{\sum_{i=1}^n 1\{c^{(i)}=j \}}$ minimizes $J$ w.r.t $ \mu$ \\
$k$-means is coordinate descent on the distortion function $J(c, u) =\sum\limits_{i=1}^n \norm{{x^{(i)}}-{\mu}_{c^{(i)}}}^2$ \\
$k$-medians is coordinate descent on $J( c,  u) =\sum_{i=1}^n \norm{{x^{(i)}}-{\mu}_{c^{(i)}}}_1$ and is more robust to outliers  \\
$k$-medoids updates cluster centroid to be ${\mu_j} = \argmin_{y \in \{x^{(1)}, \ldots, x^{(n)} \}} \sum\limits_{i=1}^n d(y, x^{(i)})$ \\
Choose number of clusters by varying $k$ and using elbow method on $J$ \\
Since $J$ is non-convex, repeat $k$-means multiple times and choose clustering with lowest distortion 

\subsection{Hierarchical Clustering}
Agglomerative clustering: bottom-up, every observation starts in its own cluster, find best pair to merge into new cluster, repeat until all clusters are merged. Produces dendrogram of clusterings \\
Single linkage: use closest pair, results in potentially long and skinny clusters \\
Complete linkage: use farthest pair, results in tight (small, round) clusters \\
Average linkage uses average of all pairs and is robust against noise (defines inter-cluster distance) \\
If $\mbox{dist}(A, B) = 2$, then create parent node $AB$ with two branches of distance $1$. Runtime $O(n^3)$

\subsection{Gaussian Mixture Models}
Assume $z^{(i)} \sim \mbox{Categorial}(\phi)$, $\phi_j = p(z^{(i)} = j)$, $\sum_{j=1}^k \phi_j = 1$, $x^{(i)} | z^{(i)} = j \sim \mathcal{N}(\mu_j, \Sigma_j)$, then \\
$\ell(\phi, \boldsymbol \mu, \boldsymbol \Sigma) = \sum\limits_{i=1}^n \log p(x^{(i)}; \phi, \boldsymbol \mu, \boldsymbol \Sigma) = \sum\limits_{i=1}^n \log \sum\limits_{z^{(i)}=1}^k p(x^{(i)}| z^{(i)}; \phi, \boldsymbol \mu, \boldsymbol \Sigma)p(z^{(i)})$, using EM \\
$r_{ij} = w^{(i)}_j = p(z^{(i)}=j; \phi, \boldsymbol \mu, \boldsymbol \Sigma) = \frac{p(x^{(i)}|z^{(i)}=j; \boldsymbol \mu, \boldsymbol \Sigma)p(z^{(i)}=j ; \phi)}{\sum_{l=1}^k p(x^{(i)}|z^{(i)}=l; \boldsymbol \mu, \boldsymbol \Sigma)p(z^{(i)}=l ; \phi)}$ [E-step] \\
${\mu_j} = \frac{\sum_{i=1}^n w^{(i)}_jx^{(i)}}{\sum_{i=1}^n w^{(i)}_j}$, ${\Sigma_{j}} = \frac{\sum_{i=1}^nw^{(i)}_j (x^{(i)} - {\mu_j})^T(x^{(i)} - {\mu_j})}{\sum_{i=1}^nw^{(i)}_j}$, $\phi_j = \frac{1}{n}\sum\limits_{i=1}^n w^{(i)}_j$ [M-step] \\
$\mbox{[E-step]}$ viewed as soft clustering examples and [M-step] viewed as weighted update of cluster centers \\
Choose number of clusters $k$ by minimizing Bayesian Information Criteria: $\ln(n)k - 2\ln(\hat L)$ 

\subsection{Cluster Metrics}
External Validation: given labels, could find weighted purity or entropy in each cluster \\
Internal Validation: without labels, clusters should be stable to subsampling of data \\
For dataset $X$, define $L(X)$ such that $L_{ij} = 1$ if $x^{(i)}$ and $x^{(j)}$ are in the same cluster else $0$ \\
Compute $\mbox{sim}(L(X), L(X^\prime))$ where $X^\prime$ is a subsample and comparison done over common examples \\
Cluster Cohesion: $\sum_{i=1}^n \norm{\boldsymbol{x^{(i)}} - \boldsymbol \mu_{c^{(i)}}}^2$, Cluster Separation: $\sum_{j=1}^k \abs{C_i} \norm{\boldsymbol \mu - \boldsymbol{\mu_j}}^2$\\
Silhouette Coefficient: for individual $x^{(i)}$, let $a = $ average distance of $x^{(i)}$ to points in its cluster and $b = \min($average distance of $x^{(i)}$ to points in another cluster$)$, want $s = 1- \sfrac{a}{b}$ close to 1 

\section{Hidden Markov Models}
Stochastic process: a collection or r.vs that evolve over time \\
Markov property: conditional distribution of future states depends only on the current state \\
A Markov chain is a stochastic process with the Markov property modeled by $\lambda = (S, \pi, A)$ and outputs observable state $O_t \in S$ such that $\P(O | \lambda) = \P(q_1, ..., q_T) = \P(q_1)\P(q_2 | q_1)...\P(q_T | q_{T-1})$ \\
Hidden Markov Models assume an underlying markov chain that generates observations   \\
$S = \{S_1, ..., S_N\}$ where the states are $q_t \in S$, $V = \{v_1, ..., v_m\}$ where the observations are $O_t \in V$ \\
$\pi = \{\pi_i\}$ where $\pi_i = \P(q_1 = S_i)$, $A = \{a_{ij}\}$ where $a_{ij} = \P(q_t = S_j | q_{t-1} = S_i)$ and \\
$B = \{b_j(k)\}$ where $b_j(k) = \P(v_k \mbox{ at t} | q_t = S_j)$ are the elements of a hidden markov model

\subsection{Forward Algorithm}
Find probability of seeing observation sequence $p(O|\lambda) = \sum_{q_T} p(q_T, O | \lambda)$  \\
Define forward variable $\alpha_t(i) = \P(O_1, O_2, ..., O_t, q_t = S_i | \lambda)$ \\
Initialize $\alpha_1(i) = \pi_i b_i(O_1)$ for $1 \leq i \leq N$ \\
Induct $\alpha_{t+1}(j) = \sum_{i=1}^N \alpha_t(i)a_{ij} b_j(O_{t+1})$ for $1 \leq t \leq T -1$ and $1 \leq j \leq N$ \\
Terminate $\P(O | \lambda) = \sum_{i=1}^N \alpha_T(i)$ with $O(N^2T)$ time and $O(NT)$ space complexity \\
Compare with brute force $p(O|\lambda) =\sum_q p(q_1)p(O_1|q_1)p(q_2|q_1) \ldots p(O_T|q_T)$ which is $O(N^TT)$

\subsection{Backward Algorithm}
Define backward variable $\beta_t(i) = \P(O_{t+1}, O_{t+2}, ..., O_T | q_t = S_i, \lambda)$ \\
Initialize $\beta_T(i) = 1$ for $1 \leq i \leq N$ \\
Induct $\beta_{t}(i) = \sum_{j=1}^N \beta_{t+1}(j) a_{ij} b_j(O_{t+1})$ for $T-1 \geq t \geq 1$ and $1 \leq i \leq N$ \\
Terminate $p(O | \lambda) = \sum_{i=1}^N \alpha_1(i)\beta_1(i) = \sum_{i=1}^N  \pi_i b_i(O_1) \beta_1(i)$

\subsection{Posterior Decoding}
Find most likely state at every point in time. Want $q^* = \{q_t | q_t = \argmax_{S_i}\P(q_t = S_i | O, \lambda)\}$ \\
Define $\gamma_t(i) = \P(q_t = S_i | O, \lambda) = \frac{\P(O, q_t = S_i | \lambda)}{\P(O | \lambda)} = \frac{\alpha_t(i)\beta_t(i)}{\sum_{i=1}^N \alpha_t(i)\beta_t(i)}$

\subsection{Viterbi Decoding}
Find most likely path $q^*$ given observation sequence. Want $q^* = \argmax_q \P(q | O, \lambda) = \argmax_q \P(q, O | \lambda)$ \\
Define Viterbi probability $\delta_t(i) = \max\limits_{q_1, ..., q_{t-1}}\P(q_1, ..., q_{t-1}, O_1, ..., O_t, q_{t} = S_i| \lambda)$ \\
Initialize $\delta_1(i) = \pi_i b_i(O_1)$ for $1 \leq i \leq N$ \\
Induct $\delta_{t+1}(j) = \big[\max\limits_{1\leq i\leq N} \delta_t(i)a_{ij}\big] b_j(O_{t+1})$, $\psi_{t+1}(j) = \argmax\limits_{1 \leq i \leq N} \delta_t(i)a_{ij}$ until $t = T-1$, $1\leq j \leq N$ \\
Terminate $p^*=\max\limits_{1\leq i \leq N} \delta_T(i)$, $q^*_T = \argmax\limits_{1 \leq i \leq N} \delta_T(i)$ with backtracking $q^*_t = \psi_{t+1}(q^*_{t+1})$ until $t=1$

\subsection{Learning HMMs}
Adjust model parameters $\lambda = (A, B, \pi)$ to maximize $\P(O | \lambda)$ \\
Given complete data (we know the underlying states). Use MLE \\
$\hat{\pi}_i = \frac{\text{Count}(q_i)}{\sum_{j=1}^N \text{Count}(q_j)}$, $\hat{a}_{ij} = \frac{\text{Count}(S_i \to S_j)}{\text{Count}(S_i)}$, $\hat{b}_j(k) = \frac{\text{Count}(S_j \to v_k)}{\text{Count}(S_j)}$ \\
Add pseudocounts to represent prior belief. Large pseudocounts $\to$ large regularization

\subsubsection{Unsupervised Learning. Baum-Welch}
Given number of states $T$, EM guaranteed to increase likelihood $p(O| \lambda)$ with complexity $O(N^2T)$ \\
Define $\xi_t(i,j) = \P(q_t = S_i, q_{t+1} = S_j | O, \lambda) = \frac{\P(q_t = S_i, q_{t+1} = S_j, O | \lambda)}{\P(O | \lambda)} = \frac{\alpha_t(S_i)a_{ij}b_j(O_{t+1})\beta_{t+1}(S_j)}{\P(O | \lambda)}$ \\
Compute $\alpha_t(i)$, $\beta_t(i)$, $\P(O| \lambda)$ using forward-backward algorithm to get $\gamma_t(i)$ and $\xi_t(i,j)$ [E-step] \\
Update model parameters: $\hat{\pi}_i = \gamma_1(i)$, $\hat{a}_{ij} = \frac{\sum_{t=1}^{T-1}\xi_t(i,j)}{\sum_{t=1}^{T-1}\gamma_t(j)}$, $\hat{b}_j(k) = \frac{\sum_{t=1 \text{ s.t } O_t = v_k}^{T} \gamma_t(j)}{\sum_{t=1}^{T}\gamma_t(j)}$ [M-step] 

\section{Learning Theory}
Let $R_n(h)$ be the training error and $R(h)$ be the expected error of $h \in \H$ 

\subsection{Haussler's Theorem}
Given finite hypothesis space $\H$, dataset $\D$ with $n$ iid training and $0 \leq \epsilon \leq 1$, then for any learned hypothesis $h$ that is consistent with the training data, $\P(R(h) > \epsilon) \leq |\H| e ^{-n\epsilon}$

\subsection{Hoeffding Inequality}
Let $Z_1, Z_2, ..., Z_n$ be $n$ iid r.vs drawn Bernoulli($\phi$) and $\hat{\phi} = \frac{1}{n}\sum_{i=1}^n Z_i$ be the mean of the $Z_i$'s \\ Then for any $\gamma > 0$, $\P(|\phi - \hat{\phi}| > \gamma) \leq 2e^{-2n\gamma^2}$ \\
Using the union bound we can derive $\P(R(h) - R_n(h) > \epsilon) \leq |\H| e^{-2n\epsilon^2}$ for any $h \in \H$

\subsection{PAC Bounds}
Suppose we are willing to tolerate at most a $\delta$ probability of having $> \epsilon$ error, then \\
$p(R(h) > \epsilon) \leq \abs \H e^{-n\epsilon} \leq \delta \iff p(R(h) \leq \epsilon) \geq 1 - \abs \H e^{-n\epsilon} \geq 1 - \delta $, so for consistent $h$ we need \\
$n \geq \frac{1}{\epsilon}(\ln |\H| + \ln \frac{1}{\delta})\mbox{ to be probably (with probability }1-\delta\mbox{) and approximately correct (with error }\epsilon)$ \\
We can also bound the true error by letting $\abs \H e^{-n\epsilon} = \delta$, in which case $\epsilon = \frac{1}{n}(\ln\abs \H + \ln \frac{1}{\delta})$ \\
For nonconsistent $h$ we have, $n \geq \frac{1}{2\epsilon^2}\big(\ln \abs \H + \ln(\frac{1}{\delta}) \big)$ and $\epsilon = \sqrt{\frac{1}{2n} \big(\ln |\H| + \ln \frac{1}{\delta} \big)}$

\subsection{Bias-Variance Tradeoff}
With probability at least $1-\delta$, $R(h) \leq R_n(h) +\sqrt{\frac{1}{2n} \big(\ln |\H| + \ln \frac{1}{\delta} \big)}$. If we increase $\abs \H$, then we can find a better $h \in \H$ to lower bias $R_n(h)$ but at the cost of higher variance $\epsilon$

\subsection{VC-Dimension}
A set $S = \{x^{(1)}, ..., x^{(m)}\}$ of points $x^{(i)} \in \X$ can be shattered by hypothesis class $\H$ if and only if for any set of labels $\{y^{(1)}, ..., y^{(m)} \}$, there exists a consistent $h \in \H$ \\
The VC-dimension over $\X$ is the size or the largest set of points in $\X$ that is shattered by $\H$ \\
$\mbox{VC}(\H) = d + 1$ where $\H$ is the set of linear classifiers with bias in $\R^d$ \\
If $|\H| = \infty$ but $\mbox{VC}(\H) = d$ in $\X$, then $R(h) \leq R_n(h) +\sqrt{\frac{1}{2n} \big[d \big(\ln \frac{2n}{d} + 1\big) + \ln \frac{4}{\delta} \big]}$ 

\subsubsection{$\mbox{VC}(\H)$ for SVMs}
Let $\H_\gamma$ be the set of classifiers in $\R^d$ with margin $\gamma$ on $\X$ where each $x^{(i)} \in \X$ satisfies $\lVert x^{(i)} \rVert \leq R$ \\
Then $\mbox{VC}(\H_\gamma) \leq \min \Big\{d, \frac{4R^2}{\gamma^2} \Big\}$


\end{document}

